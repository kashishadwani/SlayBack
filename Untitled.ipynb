{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0112275-1a20-4853-bea0-2e6985576d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "25-05-21 00:28:13 - Directory C:\\Users\\Shri\\.deepface has been created\n",
      "25-05-21 00:28:13 - Directory C:\\Users\\Shri\\.deepface\\weights has been created\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import vlc\n",
    "import threading\n",
    "from deepface import DeepFace\n",
    "import pyautogui\n",
    "import os\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d26acc7-f057-450a-b5ad-7bd4c7ffee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionVideoController:\n",
    "    def __init__(self, video_path=None):\n",
    "        self.video_path = video_path\n",
    "        self.emotion_actions = {\n",
    "            'happy': self.speed_up,\n",
    "            'surprise': self.pause_toggle,\n",
    "            'angry': self.reverse,\n",
    "            'sad': self.normal_speed,\n",
    "            'fear': self.normal_speed,\n",
    "            'disgust': self.skip_ahead,\n",
    "            'neutral': self.normal_speed,\n",
    "        }\n",
    "        \n",
    "        # VLC Instance\n",
    "        self.instance = vlc.Instance('--no-xlib')\n",
    "        self.player = self.instance.media_player_new()\n",
    "        self.is_playing = False\n",
    "        self.current_speed = 1.0\n",
    "        \n",
    "        # Face and emotion detection settings\n",
    "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        self.last_emotion = 'neutral'\n",
    "        self.emotion_cooldown = 2  # seconds between emotion actions\n",
    "        self.last_action_time = time.time()\n",
    "        \n",
    "        # Camera settings\n",
    "        self.cap = None\n",
    "        self.emotion_thread = None\n",
    "        self.running = False\n",
    "    \n",
    "    def load_video(self, video_path):\n",
    "        \"\"\"Load a video file into the player\"\"\"\n",
    "        if not os.path.exists(video_path):\n",
    "            print(f\"Error: Video file not found: {video_path}\")\n",
    "            return False\n",
    "            \n",
    "        self.video_path = video_path\n",
    "        media = self.instance.media_new(video_path)\n",
    "        self.player.set_media(media)\n",
    "        return True\n",
    "    \n",
    "    def start_playback(self):\n",
    "        \"\"\"Start video playback and emotion detection\"\"\"\n",
    "        if not self.video_path:\n",
    "            print(\"No video loaded. Please load a video first.\")\n",
    "            return\n",
    "        \n",
    "        # Start video playback\n",
    "        self.player.play()\n",
    "        self.is_playing = True\n",
    "        \n",
    "        # Start camera and emotion detection\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "        self.running = True\n",
    "        \n",
    "        # Start emotion detection in a separate thread\n",
    "        self.emotion_thread = threading.Thread(target=self.detect_emotions)\n",
    "        self.emotion_thread.daemon = True\n",
    "        self.emotion_thread.start()\n",
    "        \n",
    "        # Main video window\n",
    "        window_name = \"Emotion Video Controller\"\n",
    "        cv2.namedWindow(window_name)\n",
    "        \n",
    "        while self.running:\n",
    "            # Display camera feed in a window\n",
    "            ret, frame = self.cap.read()\n",
    "            if ret:\n",
    "                # Draw emotion text\n",
    "                cv2.putText(frame, f\"Current Emotion: {self.last_emotion}\", (10, 30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"Video Speed: {self.current_speed}x\", (10, 60), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                \n",
    "                # Show the frame\n",
    "                cv2.imshow(window_name, frame)\n",
    "            \n",
    "            # Check for key presses\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == 27:  # ESC key\n",
    "                self.stop()\n",
    "                break\n",
    "        \n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    def detect_emotions(self):\n",
    "        \"\"\"Continuously detect emotions and trigger appropriate actions\"\"\"\n",
    "        emotion_counter = {}  # Track emotion stability\n",
    "        \n",
    "        while self.running:\n",
    "            try:\n",
    "                ret, frame = self.cap.read()\n",
    "                if not ret:\n",
    "                    continue\n",
    "                \n",
    "                # Detect faces\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                faces = self.face_cascade.detectMultiScale(gray, 1.1, 5)\n",
    "                \n",
    "                if len(faces) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                # Process the largest face\n",
    "                x, y, w, h = max(faces, key=lambda rect: rect[2] * rect[3])\n",
    "                face_img = frame[y:y+h, x:x+w]\n",
    "                \n",
    "                # Analyze emotion (this is resource-intensive, so we don't do it every frame)\n",
    "                if time.time() - self.last_action_time > 0.5:  # Check every 0.5 seconds\n",
    "                    try:\n",
    "                        # Use DeepFace for emotion detection\n",
    "                        result = DeepFace.analyze(face_img, actions=['emotion'], enforce_detection=False)\n",
    "                        detected_emotion = result[0]['dominant_emotion'] if isinstance(result, list) else result['dominant_emotion']\n",
    "                        \n",
    "                        # Update emotion counter for stability\n",
    "                        emotion_counter[detected_emotion] = emotion_counter.get(detected_emotion, 0) + 1\n",
    "                        \n",
    "                        # Only trigger action if the emotion is stable for a few detections\n",
    "                        if emotion_counter.get(detected_emotion, 0) >= 3:\n",
    "                            if detected_emotion != self.last_emotion and time.time() - self.last_action_time > self.emotion_cooldown:\n",
    "                                print(f\"Detected emotion: {detected_emotion}\")\n",
    "                                self.trigger_action(detected_emotion)\n",
    "                                self.last_emotion = detected_emotion\n",
    "                                self.last_action_time = time.time()\n",
    "                                # Reset counter after action\n",
    "                                emotion_counter = {}\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in emotion detection: {e}\")\n",
    "                        \n",
    "                # Sleep to reduce CPU usage\n",
    "                time.sleep(0.1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in emotion detection thread: {e}\")\n",
    "                time.sleep(1)  # Wait a bit before retrying\n",
    "    \n",
    "    def trigger_action(self, emotion):\n",
    "        \"\"\"Trigger the appropriate action based on detected emotion\"\"\"\n",
    "        if emotion in self.emotion_actions:\n",
    "            self.emotion_actions[emotion]()\n",
    "    \n",
    "    # Video control actions\n",
    "    def speed_up(self):\n",
    "        \"\"\"Increase playback speed (happy emotion)\"\"\"\n",
    "        self.current_speed = 2.0\n",
    "        self.player.set_rate(self.current_speed)\n",
    "        print(\"Speed up to 2x\")\n",
    "    \n",
    "    def normal_speed(self):\n",
    "        \"\"\"Set normal playback speed\"\"\"\n",
    "        self.current_speed = 1.0\n",
    "        self.player.set_rate(self.current_speed)\n",
    "        print(\"Normal speed\")\n",
    "    \n",
    "    def slow_down(self):\n",
    "        \"\"\"Decrease playback speed (confused emotion)\"\"\"\n",
    "        self.current_speed = 0.5\n",
    "        self.player.set_rate(self.current_speed)\n",
    "        print(\"Slow down to 0.5x\")\n",
    "    \n",
    "    def pause_toggle(self):\n",
    "        \"\"\"Toggle pause/play (surprised emotion)\"\"\"\n",
    "        self.player.pause()\n",
    "        self.is_playing = not self.is_playing\n",
    "        print(\"Pause/Play toggled\")\n",
    "    \n",
    "    def reverse(self):\n",
    "        \"\"\"Jump back 5 seconds (angry emotion)\"\"\"\n",
    "        current_time = self.player.get_time()\n",
    "        self.player.set_time(max(0, current_time - 5000))  # Go back 5 seconds (time in ms)\n",
    "        print(\"Reverse 5 seconds\")\n",
    "    \n",
    "    def skip_ahead(self):\n",
    "        \"\"\"Skip ahead 10 seconds (bored/disgust emotion)\"\"\"\n",
    "        current_time = self.player.get_time()\n",
    "        self.player.set_time(current_time + 10000)  # Go forward 10 seconds\n",
    "        print(\"Skip ahead 10 seconds\")\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop playback and cleanup resources\"\"\"\n",
    "        self.running = False\n",
    "        if self.emotion_thread and self.emotion_thread.is_alive():\n",
    "            self.emotion_thread.join(timeout=1)\n",
    "        \n",
    "        if self.player:\n",
    "            self.player.stop()\n",
    "        \n",
    "        if self.cap and self.cap.isOpened():\n",
    "            self.cap.release()\n",
    "        \n",
    "        print(\"Playback stopped and resources released\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d0240fe-c3a6-42c3-98d6-44b85cd646b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_emotion_video_player(video_path=None):\n",
    "    \"\"\"\n",
    "    Run the emotion video player with the given video path.\n",
    "    If no path is provided, it will prompt for one.\n",
    "    \n",
    "    Parameters:\n",
    "    video_path (str, optional): Path to the video file to play\n",
    "    \"\"\"\n",
    "    # Create controller\n",
    "    controller = EmotionVideoController()\n",
    "    \n",
    "    if video_path:\n",
    "        if controller.load_video(video_path):\n",
    "            print(f\"Loaded video: {video_path}\")\n",
    "            controller.start_playback()\n",
    "        else:\n",
    "            print(\"Failed to load video. Exiting.\")\n",
    "    else:\n",
    "        # Ask user for video path\n",
    "        video_path = input(\"Enter path to video file: \")\n",
    "        if controller.load_video(video_path):\n",
    "            print(f\"Loaded video: {video_path}\")\n",
    "            controller.start_playback()\n",
    "        else:\n",
    "            print(\"Failed to load video. Exiting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8f34487-3cb7-4351-8b0a-4d14260d86c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded video: C:/Users/Shri/Downloads/videoplayback.mp4\n",
      "25-05-21 16:17:15 - facial_expression_model_weights.h5 will be downloaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://github.com/serengil/deepface_models/releases/download/v1.0/facial_expression_model_weights.h5\n",
      "To: C:\\Users\\Shri\\.deepface\\weights\\facial_expression_model_weights.h5\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5.98M/5.98M [00:27<00:00, 219kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected emotion: happy\n",
      "Speed up to 2x\n",
      "Detected emotion: neutral\n",
      "Normal speed\n",
      "Detected emotion: angry\n",
      "Reverse 5 seconds\n",
      "Detected emotion: sad\n",
      "Normal speed\n",
      "Detected emotion: neutral\n",
      "Normal speed\n",
      "Detected emotion: sad\n",
      "Normal speed\n",
      "Detected emotion: neutral\n",
      "Normal speed\n",
      "Detected emotion: sad\n",
      "Normal speed\n",
      "Detected emotion: neutral\n",
      "Normal speed\n",
      "Detected emotion: sad\n",
      "Normal speed\n",
      "Detected emotion: neutral\n",
      "Normal speed\n",
      "Detected emotion: angry\n",
      "Reverse 5 seconds\n",
      "Detected emotion: surprise\n",
      "Pause/Play toggled\n",
      "Detected emotion: neutral\n",
      "Normal speed\n",
      "Detected emotion: happy\n",
      "Speed up to 2x\n",
      "Detected emotion: neutral\n",
      "Normal speed\n",
      "Detected emotion: happy\n",
      "Speed up to 2x\n",
      "Detected emotion: angry\n",
      "Reverse 5 seconds\n",
      "Detected emotion: neutral\n",
      "Normal speed\n",
      "Detected emotion: angry\n",
      "Reverse 5 seconds\n",
      "Detected emotion: neutral\n",
      "Normal speed\n",
      "Detected emotion: angry\n",
      "Reverse 5 seconds\n",
      "Detected emotion: neutral\n",
      "Normal speed\n",
      "Detected emotion: angry\n",
      "Reverse 5 seconds\n",
      "Detected emotion: neutral\n",
      "Normal speed\n",
      "Detected emotion: happy\n",
      "Speed up to 2x\n",
      "Detected emotion: neutral\n",
      "Normal speed\n",
      "Detected emotion: angry\n",
      "Reverse 5 seconds\n",
      "Detected emotion: neutral\n",
      "Normal speed\n",
      "Detected emotion: angry\n",
      "Reverse 5 seconds\n",
      "Detected emotion: neutral\n",
      "Normal speed\n",
      "Detected emotion: angry\n",
      "Reverse 5 seconds\n",
      "Detected emotion: neutral\n",
      "Normal speed\n",
      "Detected emotion: angry\n",
      "Reverse 5 seconds\n",
      "Detected emotion: neutral\n",
      "Normal speed\n",
      "Detected emotion: angry\n",
      "Reverse 5 seconds\n",
      "Detected emotion: neutral\n",
      "Normal speed\n",
      "Detected emotion: angry\n",
      "Reverse 5 seconds\n",
      "Detected emotion: neutral\n",
      "Normal speed\n",
      "Detected emotion: angry\n",
      "Reverse 5 seconds\n",
      "Detected emotion: neutral\n",
      "Normal speed\n",
      "Playback stopped and resources released\n"
     ]
    }
   ],
   "source": [
    "run_emotion_video_player(\"C:/Users/Shri/Downloads/videoplayback.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b154f52b-3bf7-40dd-a85e-350b5f6d754d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
